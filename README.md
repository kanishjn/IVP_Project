# SDSS Astronomical Object Classification with SVM

## 🎯 Project Overview

This project implements a Support Vector Machine (SVM) classifier to identify astronomical objects from SDSS (Sloan Digital Sky Survey) FITS images into three categories:
- **GALAXY** - Extended galactic structures
- **QSO** (Quasar) - Quasi-stellar objects  
- **STAR** - Point-like stellar objects

---

## 🏆 Final Model Performance

### **Improved SVM Model** - `improved_svm_model.joblib`

#### Overall Accuracy
| Dataset | Samples | Accuracy | F1 (macro) | Status |
|---------|---------|----------|------------|--------|
| **Training Set** | 532 (balanced) | **74.44%** | 0.7443 | ✅ |
| **Test Set** | 134 (balanced) | **74.63%** | 0.7462 | ✅ Best |
| **Full Dataset** | 1000 (imbalanced) | **65.60%** | 0.6533 | ✅ |

#### Per-Class Performance (Test Set - 134 samples)
| Class | Precision | Recall | F1-Score | Accuracy | Support |
|-------|-----------|--------|----------|----------|---------|
| **GALAXY** | 0.667 | 0.773 | 0.716 | 77.3% (34/44) | 44 |
| **QSO** | 0.811 | 0.667 | 0.732 | 66.7% (30/45) | 45 |
| **STAR** | 0.783 | 0.800 | 0.791 | 80.0% (36/45) | 45 |

#### Per-Class Performance (Full Dataset - 1000 samples)
| Class | Precision | Recall | F1-Score | Accuracy | Support |
|-------|-----------|--------|----------|----------|---------|
| **GALAXY** | 0.799 | 0.574 | 0.668 | 57.4% (291/507) | 507 |
| **QSO** | 0.645 | 0.723 | 0.682 | 72.3% (196/271) | 271 |
| **STAR** | 0.509 | 0.761 | 0.610 | 76.1% (169/222) | 222 |

#### Key Metrics
- **No Overfitting**: Training (74.44%) vs Test (74.63%) - Gap: -0.19% ✅
- **Generalization**: Excellent performance on unseen balanced data
- **Inference Speed**: ~0.01ms per sample
- **Model Size**: 179 KB

---

## 🔧 Model Architecture

### Feature Engineering Pipeline
```
35 Base Features → Feature Engineering → 61 Features → SelectKBest (k=50) → Final Features
```

**Base Features (35):**
- Morphological: Area, axes, eccentricity, solidity, extent, orientation
- Intensity: Mean, max, min intensity values
- Shape descriptors: Hu moments (7)
- Texture: LBP features (10)
- Radial profile (8)

**Engineered Features (+26):**
1. **Intensity features (3)**: Range, contrast, normalization
2. **Morphological features (4)**: Compactness, circularity, shape complexity, elongation
3. **Combined features (2)**: Area-intensity product, area-perimeter ratio
4. **Log transformations (3)**: Log(area), log(intensity), log(max intensity)
5. **Radial profile statistics (10)**: Gradients, mean, std, max/min ratio
6. **Hu moments statistics (2)**: Mean, standard deviation
7. **LBP texture statistics (2)**: Entropy, uniformity

### Classification Pipeline
```python
Pipeline([
    ('selector', SelectKBest(mutual_info_classif, k=50)),
    ('scaler', StandardScaler()),
    ('svm', SVC(C=10, gamma=0.05, kernel='rbf', class_weight='balanced'))
])
```

**Hyperparameters:**
- **Kernel**: RBF (Radial Basis Function)
- **C**: 10 (regularization)
- **gamma**: 0.05 (kernel coefficient)
- **class_weight**: 'balanced' (handles class imbalance)

---

## 📁 Project Structure

```
IVP/
├── README.md                              # This file
├── README_FINAL_MODEL.md                  # Detailed model documentation
├── FINAL_RESULTS.md                       # Complete results breakdown
├── VISUALIZATION_SUMMARY.md               # Visualization guide
│
├── Core Files
│   ├── improved_svm_model.joblib          # Final trained model (179 KB)
│   ├── features.npy                       # Extracted features (1000×35)
│   ├── labels.npy                         # Class labels
│   ├── train_model_improved.py            # Training script
│   ├── final_model_evaluation.py          # Comprehensive evaluation
│   ├── create_performance_visualizations.py  # Visualization generator
│   ├── fits_converter.py                  # FITS feature extraction
│   └── preprocess_and_extract.py          # Data preprocessing
│
├── Visualizations
│   ├── per_class_accuracy_comparison.png  # Bar graph: per-class accuracy
│   ├── overall_accuracy_comparison.png    # Bar graph: overall accuracy
│   ├── precision_recall_comparison.png    # Precision/recall comparison
│   ├── final_model_confusion_train.png    # Training confusion matrix
│   ├── final_model_confusion_test.png     # Test confusion matrix
│   └── final_model_confusion_full.png     # Full dataset confusion matrix
│
└── Data
    └── sdss_data/                         # FITS images and metadata
        ├── *.fits                         # SDSS FITS files
        ├── images/*.fits                  # Additional images
        └── metadata.csv                   # Dataset metadata
```

---

## 🚀 Quick Start

### Prerequisites
```bash
pip install numpy scikit-learn matplotlib seaborn joblib scipy
```

### 1. Evaluate the Model
Run comprehensive evaluation on training, test, and full datasets:
```bash
python3 final_model_evaluation.py
```

**Output:**
- Per-class metrics for all datasets
- Confusion matrices
- Overfitting analysis
- 3 confusion matrix visualizations

### 2. Generate Performance Visualizations
Create bar graphs and comparison charts:
```bash
python3 create_performance_visualizations.py
```

**Output:**
- `per_class_accuracy_comparison.png` - Side-by-side accuracy bars
- `overall_accuracy_comparison.png` - Overall accuracy comparison
- `precision_recall_comparison.png` - Precision/recall analysis

### 3. Retrain the Model
```bash
python3 train_model_improved.py
```

**Process:**
1. Loads 1000 FITS images
2. Balances dataset (222 samples per class)
3. Engineers 61 features from 35 base features
4. Trains SVM with optimal hyperparameters
5. Saves `improved_svm_model.joblib`

---

## 📊 Model Evolution

### Development History
| Version | Accuracy (Test) | Key Changes |
|---------|----------------|-------------|
| Baseline | 57.46% | Simple SVM, no feature engineering |
| With Ensemble | 61.85% | 3 SVMs voting, 80 features, RobustScaler |
| Grid Search | 58.96% | Extensive hyperparameter search |
| SMOTE/Stacking | 55.97% | ❌ Overfitting, too complex |
| **Final (Improved)** | **74.63%** | ✅ Ultimate config: 61→50 features, StandardScaler, single SVM |

### What Worked ✅
- **Simple single SVM** outperforms ensemble approaches
- **StandardScaler** better than RobustScaler for this dataset
- **50 features** (selected from 61) is optimal
- **Balanced training data** improves minority class detection
- **Proven hyperparameters**: C=10, gamma=0.05 from extensive grid search

### What Didn't Work ❌
- Ensemble voting with 3 SVMs (overly complex)
- SMOTE oversampling (caused overfitting)
- Stacking classifier (decreased performance)
- Using all 80 features (61 with k=50 selection is better)
- Data augmentation (added noise without benefit)

---

## 📈 Key Insights

### Performance Analysis

**Strengths:**
- ✅ **Excellent generalization**: No overfitting (test ≥ train accuracy)
- ✅ **Consistent QSO detection**: 72.3% on imbalanced full dataset
- ✅ **Strong STAR detection**: 76-80% across all datasets
- ✅ **High GALAXY precision**: 79.9% (when predicted, usually correct)

**Challenges:**
- ⚠️ **GALAXY recall on full data**: Only 57.4% (misses many galaxies)
- ⚠️ **STAR precision on full data**: Only 50.9% (over-predicts stars)
- ⚠️ **GALAXY-STAR confusion**: 133 galaxies misclassified as stars

### Class Imbalance Impact
- **Balanced data (test)**: 74.63% accuracy
- **Imbalanced data (full)**: 65.60% accuracy
- **Difference**: 9.03 percentage points

This drop is expected because:
1. Model trained on balanced data (222 each)
2. Real-world data is imbalanced (507 GALAXY, 271 QSO, 222 STAR)
3. Model shows bias toward minority classes

### Common Confusion Patterns
1. **GALAXY → STAR**: Most common error (133 cases)
   - Similar morphological features
   - Overlapping intensity distributions
2. **QSO → GALAXY**: 45 cases
   - Some QSOs have extended host galaxies
3. **STAR → GALAXY**: 28 cases
   - Bright stars can appear extended

---

## 🔬 Technical Details

### Feature Selection Strategy
- **Method**: Mutual Information Classification
- **Features Selected**: 50 out of 61
- **Rationale**: Balances information content vs. dimensionality

### Training Strategy
- **Dataset**: Balanced (222 samples per class = 666 total)
- **Split**: 80% train (532), 20% test (134)
- **Cross-Validation**: 5-fold stratified
- **Random Seed**: 42 (for reproducibility)

### Model Complexity
- **Input Features**: 50 (after selection)
- **Kernel**: RBF (infinite dimensional space)
- **Support Vectors**: ~300-400 (depends on training)
- **Decision Boundary**: Non-linear

---

## 📚 Documentation

- **[README_FINAL_MODEL.md](README_FINAL_MODEL.md)** - Comprehensive model guide
- **[FINAL_RESULTS.md](FINAL_RESULTS.md)** - Detailed results with confusion matrices
- **[VISUALIZATION_SUMMARY.md](VISUALIZATION_SUMMARY.md)** - Guide to all visualizations

---

## 🎯 Results Summary

### Confusion Matrix (Test Set)
```
              Predicted
            GALAXY  QSO  STAR
  GALAXY      34     3     7     (77.3%)
  QSO         12    30     3     (66.7%)
  STAR         5     4    36     (80.0%)
```

### Confusion Matrix (Full Dataset)
```
              Predicted
            GALAXY  QSO  STAR
  GALAXY     291    83   133     (57.4%)
  QSO         45   196    30     (72.3%)
  STAR        28    25   169     (76.1%)
```

---

## 🏅 Achievements

✅ **74.63% accuracy** on balanced test set (+17.17 pp from baseline)  
✅ **65.60% accuracy** on full imbalanced dataset  
✅ **No overfitting** - excellent generalization  
✅ **Fast inference** - 0.01ms per sample  
✅ **Reproducible** - fixed random seeds  
✅ **Comprehensive evaluation** - 3 datasets, multiple metrics  
✅ **Rich visualizations** - 7 performance graphs  

---

## 📊 Visualizations

All visualizations are generated automatically and saved as high-resolution PNG files:

1. **Per-Class Accuracy Comparison** - Side-by-side bars for GALAXY/QSO/STAR
2. **Overall Accuracy Comparison** - Train/Test/Full dataset comparison
3. **Precision-Recall Analysis** - Dual graphs showing model trade-offs
4. **Confusion Matrices** - 3 matrices (train/test/full) with heatmaps

Run `python3 create_performance_visualizations.py` to regenerate all graphs.

---

## 🛠️ Dependencies

```
numpy>=1.21.0
scikit-learn>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
joblib>=1.0.0
scipy>=1.7.0
```

Optional (for preprocessing):
```
astropy>=4.3.0
opencv-python>=4.5.0
```

---

## 📞 Usage Examples

### Load and Use the Model
```python
import numpy as np
import joblib

# Load model
model_data = joblib.load('improved_svm_model.joblib')
pipeline = model_data['pipeline']
label_encoder = model_data['label_encoder']

# Load data
X = np.load('features.npy')

# Engineer features (use engineer_features function from scripts)
from train_model_improved import ImprovedSVMTrainer
trainer = ImprovedSVMTrainer()
X_engineered = trainer.engineer_advanced_features(X, verbose=False)

# Predict
predictions = pipeline.predict(X_engineered)
class_names = label_encoder.inverse_transform(predictions)

print(f"Predicted classes: {class_names}")
```

---

## 🔄 Reproducibility

All results are reproducible using:
- Fixed random seed: 42
- Exact train/test split preserved
- Documented hyperparameters
- Version-controlled scripts

---

## 📈 Future Improvements

Potential enhancements:
1. **Deep Learning**: CNN on raw FITS images
2. **Additional Features**: Spectroscopic data, multi-band photometry
3. **Class Weighting**: Custom weights for full dataset
4. **Ensemble**: Combine with other algorithms (Random Forest, XGBoost)
5. **Active Learning**: Focus on confused samples (GALAXY-STAR boundary)

---

## 📝 Citation

If you use this project, please cite:
```
SDSS Astronomical Object Classification with SVM
Model: improved_svm_model.joblib
Accuracy: 74.63% (balanced test), 65.60% (full dataset)
Generated: October 24, 2025
```

---

## 📜 License

This project is part of an academic assignment for astronomical object classification.

---

## 🙏 Acknowledgments

- **SDSS (Sloan Digital Sky Survey)** for the astronomical data
- **scikit-learn** for machine learning tools
- **Python community** for excellent scientific computing libraries

---

**Last Updated**: October 24, 2025  
**Model Version**: improved_svm_model.joblib  
**Best Test Accuracy**: 74.63%  
**Best Full Dataset Accuracy**: 65.60%
